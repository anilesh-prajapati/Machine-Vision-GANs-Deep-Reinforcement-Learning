{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence Generation"
      ],
      "metadata": {
        "id": "Fbx1-ih4E9yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this notebook we generate novel sequences of text based on a corpus of training data consisting of Shakespeare's complete works."
      ],
      "metadata": {
        "id": "FdgfL5XGFJDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "UF5bQm1uFLyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_QEyOxmRzecQ"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Data"
      ],
      "metadata": {
        "id": "S_XFNGJHE89E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1VGqnyNFXiR",
        "outputId": "a9b555ff-15a6-4b9b-88f9-a4927b4cef21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Data"
      ],
      "metadata": {
        "id": "QLGf_hFNFZ5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz6QYPIuFeF_",
        "outputId": "322d6421-ed15-41fb-9676-5f30e041c714"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a Look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnC6cN_YGQYy",
        "outputId": "03d0dadd-7b35-4720-8996-f3e430f36636"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Unique Characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thsHxI3WGceX",
        "outputId": "5403cc87-d449-4dd7-978d-326597aface2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Characters to Numerical Index Representation"
      ],
      "metadata": {
        "id": "qyoiU_CdG93m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "eVgk1gqXG7oI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "  print('  {:4s} : {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('   ....\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SER1cD4tHzpU",
        "outputId": "5a8b27bf-e604-4529-bbc2-1c3306997cae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  '\\n' :   0,\n",
            "  ' '  :   1,\n",
            "  '!'  :   2,\n",
            "  '$'  :   3,\n",
            "  '&'  :   4,\n",
            "  \"'\"  :   5,\n",
            "  ','  :   6,\n",
            "  '-'  :   7,\n",
            "  '.'  :   8,\n",
            "  '3'  :   9,\n",
            "  ':'  :  10,\n",
            "  ';'  :  11,\n",
            "  '?'  :  12,\n",
            "  'A'  :  13,\n",
            "  'B'  :  14,\n",
            "  'C'  :  15,\n",
            "  'D'  :  16,\n",
            "  'E'  :  17,\n",
            "  'F'  :  18,\n",
            "  'G'  :  19,\n",
            "   ....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Training Examples And Targets"
      ],
      "metadata": {
        "id": "abkJSPnhJ86x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create stream of characters for training dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhtTRKRbJdRb",
        "outputId": "02ae09b4-a0d8-416e-cc5d-5d53d4ba6efe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Each sequenc has 101 characters ; we'll predict next char given the preceding ones, up to 100 chars of input \n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUCUVKTnLBcc",
        "outputId": "a2e09aa9-3b06-4848-c660-3fa3296e312d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "I1N172T5xwsm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example , traget_example in dataset.take(1):\n",
        "  print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Input data: ', repr(''.join(idx2char[traget_example.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozuYtupjySXk",
        "outputId": "135c3e11-a4ca-4538-9cd1-ee31e572478f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Input data:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], traget_example[:5])):\n",
        "  print(\"Step {:4d}\".format(i))\n",
        "  print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "  print(\"  expected output: {} ({:s}\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7E4u41zy__Y",
        "outputId": "9820b9bc-3042-41e1-f15a-ce82e878e1ab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i'\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r'\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s'\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t'\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shuffle Data For Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "BUc8Xdio0dJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory . Instead, \n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sIykeFP0ZKh",
        "outputId": "d13b8385-01d3-48a4-d8a0-d6594f0d1cbe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design Neural Network Architecture"
      ],
      "metadata": {
        "id": "dvXllx1O12hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "XYdazsaR1qpK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                                batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.GRU(rnn_units,\n",
        "                          return_sequences=True,\n",
        "                          stateful=True, # Last state for index i in batch is used as initial state\n",
        "                          recurrent_initializer='glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)                    \n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "eDOnN5pb2OfK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "JVwTIWTq42Pj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5VFrNl-5AB2",
        "outputId": "cca602bf-edd7-415a-ec13-5612b633b89a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrHupqsD5Bf4",
        "outputId": "79d0294b-c19a-48b7-b58f-878150ef6ae0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "metadata": {
        "id": "zFIp1Rer5GxY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "581WrYqg5KNP",
        "outputId": "5879291b-71c9-48a9-9b71-a059ed778ce1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33, 17, 60, 26, 26,  8, 34, 21, 16, 58, 11, 53, 30, 45, 15, 61, 27,\n",
              "       22,  9, 15, 47, 62, 61, 24,  3, 22, 63, 56, 30, 41, 56, 64, 50, 23,\n",
              "       41, 17, 38, 59, 50, 62, 51,  4, 37,  3, 47, 11, 40, 58, 22, 34, 39,\n",
              "       56, 56,  9, 37,  8, 60, 26, 42, 30, 62, 11,  0, 36, 56, 35, 44, 48,\n",
              "       34, 18, 29, 14, 55, 22, 38, 30, 60,  6, 17,  5,  0, 53, 15, 61,  5,\n",
              "        5,  9, 58, 21, 21, 54, 57, 47, 64, 52, 27, 64, 62,  4,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm2qfK515MrW",
        "outputId": "85690ad1-2f5d-449e-e584-cc4f987be533"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            " \"rs do wash the blood away.\\nKeep thou the napkin, and go boast of this:\\nAnd if thou tell'st the heavy\"\n",
            "\n",
            "Next Char Predictions: \n",
            " \"UEvNN.VIDt;oRgCwOJ3CixwL$JyrRcrzlKcEZulxm&Y$i;btJVarr3Y.vNdRx;\\nXrWfjVFQBqJZRv,E'\\noCw''3tIIpsiznOzx&&\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile Model"
      ],
      "metadata": {
        "id": "OXcTlABz5bcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUW_x9Dq5UZr",
        "outputId": "8a827aa1-158d-47ab-a9b0-7d8a36cb65f5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.174514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "MylsVpjQ5foC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './model_output/seqGen'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "4k_BcHyP5lwi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train !"
      ],
      "metadata": {
        "id": "_pC7Vuyg5qeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=30"
      ],
      "metadata": {
        "id": "S5sVpSxm5nOh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history= model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ0t8oad5vh2",
        "outputId": "28cf7e36-ef0b-4125-9991-194c404d60d3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 2.6717\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.9718\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.7014\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.5495\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.4599\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.3992\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.3531\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.3144\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2800\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2472\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2148\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1836\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1508\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1172\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0837\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0483\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0113\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9764\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9416\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9066\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.8732\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.8436\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8164\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7918\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7688\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7493\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7325\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7177\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7022\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.6917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rebuild model with single output for generating text one char at a Time"
      ],
      "metadata": {
        "id": "cmNbTIe26SQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ggg8gcuR6HF1",
        "outputId": "934ad439-231c-4907-9199-c5cf211a33ef"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./model_output/seqGen/ckpt_30'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "1MKQEu6d6aGR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwVzN-1I6b30",
        "outputId": "a768fac8-95e5-45d6-872a-91777a913ed7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 256)            16640     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 65)             66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the char returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted char as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "101JOFCu6ebv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqM9aSoP6g0i",
        "outputId": "556566a0-9755-41b9-dc76-a11eacca5430"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: but me trumpet?\n",
            "\n",
            "Lord MaKus, men lions hard!\n",
            "\n",
            "LADY CAPULET:\n",
            "O me, the bloody stard and fear\n",
            "Lumbering honour on 's is the vouch.\n",
            "Come, my desires; lume! 'tis enough this well-proceedings here.\n",
            "Come, I, rotuen!\n",
            "And yet I bear a Montague, pray?\n",
            "Come, your father chances have you do but his best mars beguild now:\n",
            "Methinks thou didst brak it, some; but that\n",
            "Mine honour meanly hath left again\n",
            "I will not be amended; but that you joy were my hell.\n",
            "Abear usurp'd: pray you, away.\n",
            "\n",
            "ANGELO:\n",
            "What, art songled.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Have not the cause?\n",
            "\n",
            "MONTAGUE:\n",
            "But never took the trranmen are in arrival,\n",
            "His tempter or more benefit\n",
            "which I but weak bring to be wit, some other.\n",
            "But when for sweet is this woman.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Unto miser eyes liege.\n",
            "\n",
            "Both:\n",
            "Well, being on him?\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Then further, divers unexperl'd; and then have wounted 'go\n",
            "My prius applared my mother cowague,\n",
            "And here it is.\n",
            "I did betider,\n",
            "And to misut was the Duke of Hereford?\n",
            "\n",
            "LEONTES:\n",
            "Well, Come:\n",
            "Hast the offer service\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"HAMLET: To be, or not to be: that is the question:\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HITDgTU46kTv",
        "outputId": "7a212866-ad7a-4ddb-8d0e-411696915272"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HAMLET: To be, or not to be: that is the question: I will live,\n",
            "That, all; as you can swift again\n",
            "Where every weak sanctued and sayisfel,\n",
            "As may appear what I will hear me.\n",
            "If I put forth to credit bloods and that word's idle:\n",
            "I give a state all things in: and let the clouds,\n",
            "as beggar's sake, but stops he open arry;\n",
            "Which, puts it much distraved born.\n",
            "\n",
            "CLYORK:\n",
            "I said so haste; 'tis nursed: so you are?\n",
            "\n",
            "PAULINA:\n",
            "So mumble me, Isabel, trust me, desperate, it straight:\n",
            "Tell us, though rest above him hence in haze\n",
            "The shapen rotten fews I bold with;\n",
            "For me to cross this well-pestir, neither will I strive I give him sacreft.\n",
            "\n",
            "FLORIZEL:\n",
            "Why holingland's dagger on ren\n",
            "They did request in question am I the duke.\n",
            "\n",
            "GLOUCESTER:\n",
            "Then let's awake him hence in have seems,\n",
            "Or brother witchcrofth is proug withterous;\n",
            "well the tribunes of the justice,\n",
            "How he might I,\n",
            "Thou hast forced to seem them that did usurd\n",
            "And prove confession, doubting hence,\n",
            "And your woss banish'd; for our own bear heart\n",
            "That alter'd without to-night; say this isle all superfl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-pufuSz6lmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}